---
apiVersion: tekton.dev/v1
kind: Task
metadata:
  name: pull-disk-images
  labels:
    app.kubernetes.io/version: "0.2.0"
  annotations:
    tekton.dev/pipelines.minVersion: "0.12.1"
    tekton.dev/tags: release
spec:
  description: >-
    Tekton task to pull disk images from quay using oras pull
  params:
    - name: snapshotPath
      type: string
      description: Path to the JSON string of the Snapshot spec in the data workspace
    - name: concurrentLimit
      type: string
      description: The maximum number of images to be pulled at once
      default: 3
  workspaces:
    - name: data
      description: |
        The workspace where the disk images will be written to. This workspace should be large (on the order
        of 100G).
  steps:
    - name: pull-images
      image:
        quay.io/konflux-ci/release-service-utils:e633d51cd41d73e4b3310face21bb980af7a662f
      script: |
        #!/usr/bin/env bash
        set -eux

        SNAPSHOT_FILE="$(workspaces.data.path)/$(params.snapshotPath)"
        if [ ! -f "${SNAPSHOT_FILE}" ] ; then
            echo "No valid snapshot file was provided."
            exit 1
        fi

        # Declare a location in the same subdirectory as the pipeline CRs to save the images to
        DISK_IMAGE_DIR="$(dirname ${SNAPSHOT_FILE})"

        process_component() { # Expected argument is [component json]
            COMPONENT=$1
            PULLSPEC=$(jq -r '.containerImage' <<< "${COMPONENT}")
            DESTINATION="${DISK_IMAGE_DIR}/$(jq -r '.staged.destination' <<< "${COMPONENT}")/FILES"
            mkdir -p "${DESTINATION}"
            DOWNLOAD_DIR=$(mktemp -d)
            cd $DOWNLOAD_DIR
            # oras has very limited support for selecting the right auth entry,
            # so create a custom auth file with just one entry
            AUTH_FILE=$(mktemp)
            select-oci-auth "${PULLSPEC}" > "$AUTH_FILE"
            oras pull --registry-config "$AUTH_FILE" "$PULLSPEC"
            NUM_MAPPED_FILES=$(jq '.staged.files | length' <<< ${COMPONENT})
            for ((i = 0; i < $NUM_MAPPED_FILES; i++)) ; do
                FILE=$(jq -c --arg i "$i" '.staged.files[$i|tonumber]' <<< $COMPONENT)
                SOURCE=$(jq -r '.source' <<< $FILE)
                FILENAME=$(jq -r '.filename' <<< $FILE)
                gzip -d "${SOURCE}.gz" || echo "didn't find mapped file: ${SOURCE}.gz"
                DESTINATION_FILE="${DESTINATION}/${FILENAME}"
                # Albeit a very small one, this is a race condition since this is run in parallel
                if [ -f "${DESTINATION_FILE}" ] ; then
                    echo -n "Multiple files use the same destination value: $DESTINATION"
                    echo " and filename value: $FILENAME. Failing..."
                    exit 1
                fi
                mv $SOURCE "${DESTINATION_FILE}"
            done
        }

        N=$(params.concurrentLimit)  # The maximum number of images to be pulled at once
        RUNNING_JOBS="\j" # Bash parameter for number of jobs currently running
        NUM_COMPONENTS=$(jq '.components | length' ${SNAPSHOT_FILE})
        # Process each component in parallel
        for ((i = 0; i < $NUM_COMPONENTS; i++)) ; do
            COMPONENT=$(jq -c --arg i "$i" '.components[$i|tonumber]' "${SNAPSHOT_FILE}")
            # Limit batch size to concurrent limit
            while (( ${RUNNING_JOBS@P} >= N )); do
                wait -n
            done
            process_component $COMPONENT &
        done

        # Wait for remaining processes to finish
        while (( ${RUNNING_JOBS@P} > 0 )); do
            wait -n
        done
